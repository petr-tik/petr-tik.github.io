<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="My blog, ">

        <link rel="alternate"  href="/feeds/all.atom.xml" type="application/atom+xml" title="My blog Full Atom Feed"/>

        <title>Improving the take home assignment // My blog // </title>


    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/theme/css/pure.css">
    <link rel="stylesheet" href="/theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>
    <div class="pure-g-r" id="layout">
        <div class="sidebar pure-u">
            <div class="cover-img" style="background: none repeat scroll 0% 0% #3D4F5D">
                <div class="cover-body">
                    <header class="header">
                        <hgroup>
                            <h1 class="brand-main"><a href="">My blog</a></h1>
                            <p class="tagline"></p>
                                <p class="social">
                                    <a href="https://github.com/petr-tik">
                                        <i class="fa fa-GitHub fa-3x"></i>
                                    </a>
                                    <a href="https://twitter.com/petr_tik">
                                        <i class="fa fa-Twitter fa-3x"></i>
                                    </a>
                                </p>
                        </hgroup>
                    </header>
                </div>
            </div>
        </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Improving the take home assignment</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="/tag/interview.html">interview</a>
                        </p>
                </header>
            </section>
            <p>Below is my suggestion on the format and assessment criteria for a take-home programming assignment. Companies are starting to recognise the lack of signal in algorithmic puzzles on HackerRank/over skype as a way of assessing candidates' programming proficiency. Some companies have introduced a take-home assignment. While it can still be an algorithmic puzzle, most companies give a more representative problem closer to their problem domain.  Take home assignments have their disadvantages. They demand more time from candidates to complete and interviewers to mark, extend the interview process. However, they also allow candidates to use their own IDE, keyboard, google necessary information and take more time to think. For the rest of the article, we will assume that a good take-home assignment deserves candidate's time and attention and will discuss how to make it even better. </p>
<h2>If it ain't broken, why are you fixing it?</h2>
<p>Take-home puzzles in their current form still suffer from 2 problems. The candidate is asked to write a solution from scratch. The solution is assessed as a self-contained piece of code that isnâ€™t used afterwards. In professional software development, programmers seldom write self-contained programmes from scratch. </p>
<p>The best candidates can read, refactor and write code that is easy for others to understand.</p>
<h1>Assignment</h1>
<p>Use the advantages of a take-home test - allow candidates to take their time, google and research the problem, use own IDE/environment. </p>
<h2>Future proofing</h2>
<p>You should expect your onsite question (and everything else) to leak online - glassdoor, hackernews, reddit, friends. You have a good chance of hiding your exact measurement criteria. </p>
<p>In my opinion, NDAs and other ways of preventing are like bicycle locks. They do not guarantee 100% safety. Candidates are less likely to upload problems that are hard to upload. Leaking a real programming assignment requires starting a git repo, pushing to it and writing up a readme to make it discoverable. It's easier to add another algorithm puzzle to Leetcode after your Facebook interview.</p>
<p>Future candidates may still find the assignment and solution on GitHub/Bitbucket. Programmers shouldn't be punished for their ability to search the internet. </p>
<p>However, we should fail plagiarised submissions. Since we will be storing all submissions in our database, we can run a quick plagiarism checker on every new submission. </p>
<h2>Problem statement.</h2>
<p>Send candidates an exercise to refactor an existing codebase to comply with a new spec or API. The original implementation has more endpoints and methods required than the final. </p>
<p>We are testing the candidate's ability to read and understand someone else's code; refactor it to comply with new functionality, while reducing overall complexity. </p>
<p>Overall, a good solution should delete more lines of code than it adds, remove now-unnecessary interfaces, add new methods while maintaining test coverage and resilience to bad inputs. </p>
<h2>Process</h2>
<p>The submissions is assessed in 2 stages: computer scoring and human review of the code. </p>
<p>Computer scoring runs in a CI-like automated fashion with an environment consistent across languages and tools. Results include raw counts across several rubrics and the composite score.</p>
<p>The reviewer can't see the computer score before submitting their assessment.</p>
<p>The reviewer marks how easy it is to extend the code base. </p>
<h2>Computer scoring facets</h2>
<h3>Functionality</h3>
<p>% of unit tests passed
% of integration tests passed. Call new API endpoints with documented and undocumented inputs. 
% old endpoints that respond but shouldn't exist anymore. Out of responding old endpoints - % that have correct results.</p>
<h3>Reliability/resilience</h3>
<p>Test coverage 
Error handling - % of API endpoints crashed by malformed inputs. The original code sent to the candidate should mention fuzzing and include minimum fuzzing infrastructure. This should hint to the candidates that their API endpoints will be fuzzed. Also, including fuzzing infra makes it easy f
or candidates to implement new fuzzing methods without wasting time and adding new dependencies. </p>
<h3>Performance</h3>
<p>Measure the average and deviation of the following runs:</p>
<div class="highlight"><pre><span></span>* Process 100,000 correct requests.

* Process 100,000 malformed requests.

* Process 50,000 correct and 50,000 malformed requests in random order.
</pre></div>


<h3>Style/coherence</h3>
<div class="highlight"><pre><span></span>* Ratio of inserted vs deleted lines
</pre></div>


<p>The submission will come in form of a git repo. Candidates will be expected to commit their changes and send it back. Get the number of lines inserted and deleted in a git diff between the first commit and HEAD. </p>
<p>We are looking for a ratio as high as possible. </p>
<div class="highlight"><pre><span></span>* Statistics on variable, method and class names
</pre></div>


<p>Extract names of all variables, methods and classes in the solution. Calculate their min, max, average and standard deviation.</p>
<p>We can increase difficulty by making original function or variable names purposefully short. </p>
<div class="highlight"><pre><span></span>* Ratio of changed lines per commits
</pre></div>


<p>It's possible to make many small commits that make unrelated changes. The human reviewer will simulate code review by assessing the quality of commits.</p>
<div class="highlight"><pre><span></span>* Number of linter warnings
</pre></div>


<p>Run a linter on the whole project and record the summary of errors and warnings. The configuration of the linter should be the same with the linters you use on your code base. For Python use something like Pylint, which returns different types of messages. </p>
<p>You can write code that causes the linter to report errors on purpose before sending it to candidates. If you do that, you should add the linter as a clear dependency, otherwise you might be punishing candidates for unfamiliarity with your toolset. </p>
<p>You shouldn't compare the number of linter warnings in Java submissions against that in Python submissions. The results should only be compared with other submissions in the same language. </p>
<div class="highlight"><pre><span></span>* Documentation
</pre></div>


<p>Count how many classes and methods have a docstring. 
If you care about assessing candidates' writing ability, extract the text in the docstrings and calculate its <a href="https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests">readability</a> score. </p>
<div class="highlight"><pre><span></span>* Ratio of LoC/methods
</pre></div>


<p>Using source-code analysis to calculate the average lines of code in all methods/functions in the submission. </p>
<h3>Passing or failing candidates</h3>
<p>Using a formula or a flow chart, your company can decide how to pass or fail candidates after collecting statistics from their submissions. </p>
<p>Ideally, this should be based on the submissions completed by your engineers without prior knowledge of the test or assessment criteria. </p>
<p>Failed submissions should get a rejection email. Using the collected statistics, you can generate reasons for rejecting them. You should expect the email to be forwarded to their friends, so be careful from a legal point of view. Also, you don't want your criteria to leak online, so it's best not to give exact details. </p>
<h2>Human review</h2>
<p>The interviewer needs to assess how easy it is to work with the new code. </p>
<p>Human reviewer needs to add another endpoint that calls into methods that the candidate has added. </p>
<p>The human reviewer cannot see the exact statistics from the previous section to reduce biases.</p>
<p>However, there are many other factors that can affect the human reviewer. After grading dozens of submissions, the reviewer is now more proficient with the problem. This makes them capable of extending submissions regardless of the quality of the code. As a result, human assessment becomes less valuable. </p>
<p>This is the most subjective part of the interview so far. Given all this and other factors, I wouldn't allow human discretion to fail submissions. </p>
<h3>Additional metrics</h3>
<p>Like in a code review, one of the engineers on the team looks at the diffs. This assesses the semantic coherence of every commit.</p>
<h2>Onsite follow up</h2>
<p>Bring the candidate onsite and ask them to pair programme with someone from the team. In the pair programming exercise, ask them to extend their original submission to include new functionality. </p>
<p>Asking candidates to code onsite will confirm that they did the assignment themselves. It also makes them familiar with the code base and their tools, reducing their nervousness. </p>
<p>Below is a list of pros and cons of the suggested approach. </p>
<h1>Benefits</h1>
<ol>
<li>Even closer to real-world software development. Assesses candidates' ability to understand code, solve problems and deliver software that is easy to extend. </li>
<li>Systematic, bias-free assessment across clear predetermined criteria. </li>
<li>Ability to score previous candidates' solutions and keep grading fair for everyone.</li>
</ol>
<p>Keeping a history of all previous submissions allows us to add new metrics. After adding the new metric to the assessment framework, rerun it on all submissions to add new score.</p>
<ol>
<li>Assess different aspects with the same test. </li>
</ol>
<p>Different teams in the same company can use the same exercise but prioritise one rubric over all others. For example, candidates applying for a role in the performance-focussed team will be expected to submit solutions with above-average performance. </p>
<p>Collecting standardised, consistent metrics across different dimensions enables the interviewers to avoid bias and get a detailed outline of candidate's skills. </p>
<h1>Disadvantages</h1>
<ol>
<li>
<p>More time consuming for the candidate. </p>
<ol>
<li>Too close to real-world software engineering. </li>
</ol>
</li>
</ol>
<p>Candidates are people too, they want to relax in their free time, not read and write more code. </p>
<div class="highlight"><pre><span></span>2. This might scare off passive candidates.
</pre></div>


<p>Some candidates might be interested in the company, but will drop out of the process, if asked to do a big take-home assignment. </p>
<h3>Possible solutions</h3>
<div class="highlight"><pre><span></span>* Make sure the test can be completed well in the advertised time.
</pre></div>


<p>Ask 3-5 developers in your company to do it and measure how long it took them.</p>
<p>Picard management tip: tell candidates "X of your prospective team mates did it and it took them {Y} hours".format(Y=average_time*1.5). You are better off scaring them at the start. If you lure them in with a promise of a couple of hours, they will end up feeling disappointed with themselves and you. </p>
<div class="highlight"><pre><span></span>* Sell the company and role to the candidate.
</pre></div>


<p>Pitch the company and the role enough for them to see it through. The point of a good assignment is to make sure they can program, before you bring them onsite. Cynically enough, you are expecting most applicants to fail here. It doesn't hurt to pull out all the stops and sell the company really hard before the take home test. A good sell should give candidates enough motivation to finish it. </p>
<div class="highlight"><pre><span></span>* Consider paying the candidates a nominal amount for their time.
</pre></div>


<p>They are not doing actual work for your company, so they shouldn't be paid as if they are working. They are doing a small, representative programming assignment and should be paid around 20-50 GBP. This won't work at Google-scale of millions of applicants per year. If your company gets 1000 applicants per year, you must spend that order of magnitude on fairs, flyers, advertising and agencies. The additional cost will make your application process stand out and help you collect valuable data from a wide pool of applicants. </p>
<ol>
<li>The assessment criteria require language-specific dependencies. </li>
</ol>
<p>Implementing the problem and the assessment infrastructure</p>
<p>This will filter out candidates that don't feel proficient enough in the language(s) offered. </p>
<h3>Possible solutions</h3>
<div class="highlight"><pre><span></span>* Invest upfront into grading framework for 2 or 3 main languages that your company uses.
</pre></div>


<p>If you use more languages than you can assess submissions in, maybe you should ask yourself why are you using that many languages in the first place. </p>
<ol>
<li>If assessment criteria leak online, it might skew measurements. </li>
</ol>
<p>According to Goodhart's law once a measure becomes a target, it ceases to be a good measure. For example, if we tell candidates that we calculate the ratio of inserted lines over deleted lines, they will start playing code golf and cutting corners to improve their ratio. </p>
<h3>Possible solutions</h3>
<div class="highlight"><pre><span></span>* Devise criteria to catch transgressions across each other
</pre></div>


<p>In the example above, the candidate playing code golf wrt lines of code might resort to single letter variable names. Since we will also be measuring that, improving the lines metric is likely to hurt the length of names metric. </p>
<div class="highlight"><pre><span></span>* Tell candidates about the criteria in general.
</pre></div>


<p>Warn the candidates that their submissions runs through an automatic assessment across several rubrics like testing, style and performance. This gives everyone taking part fair warning. At the same time, the candidate is free to decide on the trade-off between time spent vs quality of submission. Applicants with families and other commitments cannot spend as much time as recent graduates. At the same time, mature professionals should be better at assessing the time/quality trade-off than someone coming out of university.</p>
<h1>Conclusion</h1>
<p>Giving developer candidates a take-home test is a more realistic way to gauge their programming ability. Reading and modifying already existing code and making sure it can be extended by someone else is one of the key skills for a programmer. We can improve a) type of programming assignment b) breadth and quality of metrics collected from each assignment. Overall, </p>
            <a href="#" class="go-top">Go Top</a>
<footer class="footer">
    <p>&copy; Petr Tikilyaynen &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
    </div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>

</body>
</html>