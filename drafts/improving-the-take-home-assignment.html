<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Articles, ">

        <link rel="alternate"  href="/feeds/all.atom.xml" type="application/atom+xml" title="Articles Full Atom Feed"/>

        <title>Improving the take home assignment // Articles // </title>


    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/theme/css/pure.css">
    <link rel="stylesheet" href="/theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>
    <div class="pure-g-r" id="layout">
        <div class="sidebar pure-u">
            <div class="cover-img" style="background: none repeat scroll 0% 0% #3D4F5D">
                <div class="cover-body">
                    <header class="header">
                        <hgroup>
                            <h1 class="brand-main"><a href="">Articles</a></h1>
                            <p class="tagline"></p>
                                <p class="social">
                                    <a href="https://github.com/petr-tik">
                                        <i class="fa fa-GitHub fa-3x"></i>
                                    </a>
                                    <a href="https://twitter.com/petr_tik">
                                        <i class="fa fa-Twitter fa-3x"></i>
                                    </a>
                                </p>
                        </hgroup>
                    </header>
                </div>
            </div>
        </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Improving the take home assignment</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="/tag/interview.html">interview</a>
                        </p>
                </header>
            </section>
            <p>Assessing software developers using algorithmic puzzles on HackerRank/skype is proving difficult. Companies aren't assessing the skills relevant to the jobs and candidates are complaining about this. </p>
<p>Some companies have introduced a take-home assignment. Take home assignments have their disadvantages. They demand more time from candidates to complete and interviewers to mark, which extends the interview process. However, they also allow candidates to use their own IDE, keyboard, google necessary information and take more time to think. For the rest of the article, we will assume that a good take-home assignment deserves candidate's time and attention and will discuss how to make it even better. </p>
<h2>If take home tests aren't broken, why are you fixing them?</h2>
<p>Take-home puzzles in their current form still suffer from 2 problems. Firstly, the candidate is asked to write a solution from scratch. Secondly, the solution is assessed as a self-contained piece of code that isnâ€™t used afterwards. In professional software development, programmers seldom write self-contained programmes from scratch that aren't refactored afterwards.</p>
<p>Iant to make a better method of take home testing that assesses those 2 qualities in candidates.</p>
<h1>Assignment</h1>
<p>Use the advantages of a take-home test - allow candidates to take their time, google and research the problem, use own IDE/environment. </p>
<h2>Future proofing</h2>
<p>Expect your onsite question (and everything else) to leak online - glassdoor, hackernews, reddit, friends. You have a good chance of hiding your exact measurement criteria. </p>
<p>In my opinion, NDAs and other ways of preventing are like bicycle locks. They do not guarantee 100% safety. Candidates are less likely to upload problems that are hard to upload. Leaking a real programming assignment requires starting a git repo, pushing to it and writing up a readme to make it discoverable. It's easier to add another algorithm puzzle to Leetcode after your Facebook interview.</p>
<p>Future candidates may still find the assignment and solution on GitHub/Bitbucket. Programmers shouldn't be punished for their ability to search the internet. </p>
<p>Since we will be storing all submissions in our database, we can check new submissions against the corpus and fail plagiaristic submissions. </p>
<h2>Problem statement</h2>
<p>Send candidates an exercise to refactor an existing codebase to comply with a new spec or API. The original implementation has more endpoints and methods required than the final. </p>
<p>We are testing the candidate's ability to read and understand someone else's code; refactor it to comply with new functionality, while reducing overall complexity. </p>
<p>Overall, a good solution should delete more lines of code than add, remove now-unnecessary interfaces, add new methods while maintaining test coverage and resilience to bad inputs. </p>
<h2>Assessment</h2>
<p>The submissions is assessed in 2 stages: computer scoring and human review of the code. </p>
<p>Computer scoring runs in a CI-like automated fashion with an environment consistent across languages and tools. Results include raw counts across several rubrics and the composite score.</p>
<p>The reviewer marks how easy it is to extend the code base. </p>
<h2>Computer scoring facets</h2>
<h3>Functionality</h3>
<ul>
<li>
<p>% of unit tests passed</p>
</li>
<li>
<p>% of integration tests passed. Call new API endpoints with documented and undocumented inputs. </p>
</li>
<li>
<p>% old endpoints that respond but shouldn't exist anymore. Out of responding old endpoints - % that have correct results.</p>
</li>
</ul>
<h3>Reliability/resilience</h3>
<ul>
<li>
<p>Test coverage </p>
</li>
<li>
<p>Error handling - % of API endpoints crashed by malformed inputs. The original code sent to the candidate should mention fuzzing and include minimum fuzzing infrastructure. This should hint to the candidates that their API endpoints will be fuzzed. Also, including fuzzing infra makes it easy f
or candidates to implement new fuzzing methods without wasting time and adding new dependencies. </p>
</li>
</ul>
<h3>Performance</h3>
<p>Measure the average and deviation of the following runs:</p>
<ul>
<li>
<p>Process 100,000 correct requests. </p>
</li>
<li>
<p>Process 100,000 malformed requests.</p>
</li>
<li>
<p>Process 50,000 correct and 50,000 malformed requests in random order.</p>
</li>
</ul>
<h3>Style/coherence</h3>
<ul>
<li>Ratio of inserted vs deleted lines </li>
</ul>
<p>The submission will come in form of a git repo. Candidates will be expected to commit their changes and send it back. Get the number of lines inserted and deleted in a git diff between the first commit and HEAD. </p>
<p>We are looking for a ratio as high as possible. </p>
<ul>
<li>Statistics on variable, method and class names</li>
</ul>
<p>Extract names of all variables, methods and classes in the solution. Calculate  min, max, average and standard deviation of their lengths.</p>
<p>Make it more difficult by giving the functions and variables purposefully short names, before you send it to the candidate. </p>
<ul>
<li>Ratio of changed lines per commits </li>
</ul>
<p>It's possible to make many small commits that make unrelated changes. The human reviewer will simulate code review by assessing the quality of commits.</p>
<ul>
<li>Number of linter warnings</li>
</ul>
<p>Run a linter on the whole project and record the summary of errors and warnings. The configuration of the linter should be the same with the linters you use on your code base. For Python use something like Pylint, which returns different types of messages. </p>
<p>You can write code that causes the linter to report errors on purpose before sending it to candidates. If you do that, you should add the linter as a clear dependency, otherwise you might be punishing candidates for unfamiliarity with your toolset. </p>
<p>You shouldn't compare the number of linter warnings in Java submissions against that in Python submissions. The results should only be compared with other submissions in the same language. </p>
<ul>
<li>Documentation</li>
</ul>
<p>Count how many classes and methods have a docstring. 
If you care about assessing candidates' writing ability, extract the text in the docstrings and calculate its <a href="https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests">readability</a> score. </p>
<ul>
<li>Ratio of LoC/methods</li>
</ul>
<p>Using source-code analysis to calculate the average lines of code in all methods/functions in the submission. </p>
<h3>Passing or failing candidates</h3>
<p>Using a formula or a flow graph, your company can decide how to pass or fail candidates after collecting statistics from their submissions.</p>
<p>The most fair comparison is against the submissions completed by your engineers without prior knowledge of the test or assessment criteria.</p>
<p>Failed submissions should get a rejection email. Generate rejection summary using the collected statistics. You don't want your criteria to leak online, so it's best not to give exact details. </p>
<h2>Human review</h2>
<p>The interviewer assesses how easy it is to work with the candidate's code.</p>
<p>Human reviewer needs to add another endpoint that calls into methods that the candidate has added. </p>
<p>The human reviewer cannot see the exact statistics from the previous section to reduce biases.</p>
<p>However, there are many other factors that can affect the human reviewer. After grading dozens of submissions, the reviewer is now more proficient with the problem. This makes them capable of working with the code regardless of the quality of the submission.</p>
<p>This is the most subjective part of the interview so far. Given all this and other factors, I wouldn't allow human discretion to fail submissions. </p>
<h2>Onsite follow up</h2>
<p>If the candidate goes onsite and ask them to pair programme with someone from the team. Their task is to extend their original submission to include new functionality. </p>
<h2>Summary of trade-offs</h2>
<p>Below is a list of pros and cons of the suggested approach. </p>
<h1>Benefits</h1>
<ol>
<li>Even closer to real-world software development. Assesses candidates' ability to understand code, solve problems and deliver software that is easy to extend. </li>
<li>Systematic, bias-free assessment across clear predetermined criteria. </li>
<li>Ability to score previous candidates' solutions and keep grading fair for everyone.</li>
</ol>
<p>Keeping a history of all previous submissions allows us to add new metrics. After adding the new metric to the assessment framework, rerun it on all submissions to add new score.</p>
<ol>
<li>Assess different aspects with the same test. </li>
</ol>
<p>Different teams in the same company can use the same exercise but prioritise one rubric over all others. For example, candidates applying for a role in the performance-focussed team will be expected to submit solutions with above-average performance. </p>
<p>Collecting standardised, consistent metrics across different dimensions enables the interviewers to avoid bias and get a detailed outline of candidate's skills. </p>
<h1>Disadvantages</h1>
<ol>
<li>
<p>More time consuming for the candidate. </p>
<ol>
<li>Too close to real-world software engineering. </li>
</ol>
</li>
</ol>
<p>Candidates are people too, they want to relax in their free time, not read and write more code. </p>
<div class="highlight"><pre><span></span>2. This might scare off passive candidates.
</pre></div>


<p>Some candidates might be interested in the company, but will drop out of the process, if asked to do a big take-home assignment. </p>
<h3>Possible solutions</h3>
<ul>
<li>Make sure the test can be completed well in the advertised time. </li>
</ul>
<p>Ask 3-5 developers in your company to do it and measure how long it took them.</p>
<p>Picard management tip: tell candidates "X of your prospective team mates did it and it took them {Y} hours".format(Y=average_time*1.5). You are better off scaring them at the start. If you lure them in with a promise of a couple of hours into a test that took your team half a day, candidates will end up feeling disappointed with themselves and angry at you. </p>
<ul>
<li>Sell the company and role to the candidate before the test. </li>
</ul>
<p>Organise a phone call to pitch the company and the role enough for them to see it through. A good sell should give candidates enough motivation to finish the assignment. Cynically enough, you are expecting most applicants to fail here. It doesn't hurt to pull out all the stops and sell the company really hard before the take home test. This carries a secondary effect of making a great impression even on the people, who you aren't employing can help you hire their friend. </p>
<ul>
<li>Consider paying the candidates a nominal amount for their time. </li>
</ul>
<p>They are not doing actual work for your company, so they shouldn't be paid as if they are working. They are doing a small, representative programming assignment and should be paid around 20-50 GBP. If your company gets 1000 applicants per year, you must spend that order of magnitude on fairs, flyers, advertising and agencies. The additional cost will make your application process stand out and help you collect valuable data from a wide pool of applicants. </p>
<p>This won't work at Google-scale of millions of applicants per year.</p>
<ol>
<li>The assessment criteria require language-specific dependencies. </li>
</ol>
<p>Implementing the problem and the assessment infrastructure</p>
<p>This will filter out candidates that don't feel proficient enough in the language(s) offered.</p>
<h3>Possible solutions</h3>
<ul>
<li>Invest upfront into grading framework for 2 or 3 main languages that your company uses. </li>
</ul>
<p>If you use more languages than you can assess submissions in, maybe you should ask yourself why are you using that many languages in the first place. </p>
<ul>
<li>If assessment criteria leak online, it might skew measurements. </li>
</ul>
<p>According to Goodhart's law once a measure becomes a target, it ceases to be a good measure. For example, if we tell candidates that we calculate the ratio of inserted lines over deleted lines, they will start playing code golf and cutting corners to improve their ratio. </p>
<h3>Possible solutions</h3>
<ul>
<li>Devise criteria to catch transgressions across each other</li>
</ul>
<p>In the example above, the candidate playing code golf wrt lines of code might resort to single letter variable names. Since we will also be measuring that, improving the lines metric is likely to hurt the length of names metric. </p>
<ul>
<li>Tell candidates about the criteria in general. </li>
</ul>
<p>Warn the candidates that their submissions run through an automatic assessment across several rubrics like testing, style and performance. This gives participants fair warning. At the same time, the candidate is free to decide on the trade-off between time spent vs quality of submission. </p>
<h1>Conclusion</h1>
<p>Giving developer candidates a take-home test is a more realistic way to gauge their programming ability. Reading and modifying already existing code and making sure it can be extended by someone else is one of the key skills for a programmer. We can improve a) type of programming assignment b) breadth and quality of metrics collected from each assignment. Overall, </p>
            <a href="#" class="go-top">Go Top</a>
<footer class="footer">
    <p>&copy; Petr Tikilyaynen &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
    </div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>

</body>
</html>