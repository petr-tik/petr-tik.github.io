<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en-us">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1"/>

<title>Improving the take home assignment | Software and puns</title>



<link href="http://petr-tik.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Software and puns" />

<link rel="stylesheet" href="/css/style.css"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
<section class="section">
  <div class="container">
    <nav class="nav">
      <div class="nav-left">
        <a class="nav-item" href="http://petr-tik.github.io/">
          <h1 class="title is-4">Software and puns</h1>
        </a>
      </div>
      <div class="nav-right">
        <nav class="nav-item level is-mobile"><a class="level-item" aria-label="github" href='https://github.com/petr-tik'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="twitter" href='https://twitter.com/petr_tik'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"/>
    
  </svg></i>
            </span>
          </a></nav>
      </div>
    </nav>

    <nav class="nav">
      
      <div class="nav-left"><a class="nav-item" href="/about">
          <h2 class="title is-5">whoami</h2>
        </a></div>
      

      
    </nav>

  </div>
</section>
<section class="section">
  <div class="container">
    <div class="subtitle tags is-6 is-pulled-right">
      
      
<a class="subtitle is-6" href="/tags/interviewing">#interviewing</a>



      
    </div>
    <h2 class="subtitle is-6">May 15, 2019</h2>
    <h1 class="title">Improving the take home assignment</h1>
    
    <div class="content">
      

<p>Interviewing software developers using algorithmic puzzles on HackerRank/skype is proving difficult. People feel that companies aren&rsquo;t assessing the relevant skills. This drives applicants away from certain companies.</p>

<p>Some companies use a take home assignment instead.</p>

<p>Take home tests have their disadvantages. They demand more time from candidates to complete and interviewers to mark, which extends the interview process.</p>

<p>However, they also allow candidates to use their own IDE, keyboard, google necessary information and think about the problem without time pressure.</p>

<p>In this article, I assume that a good take home assignment deserves candidates&rsquo; time and attention and outline how to make the assignment useful to interviewers and enjoyable to interviewees.</p>

<h2 id="if-take-home-tests-aren-t-broken-why-fix-them">If take home tests aren&rsquo;t broken, why fix them?</h2>

<p>Take home tests in their current form still suffer from 2 problems.</p>

<p>Firstly, the candidate is asked to write a solution from scratch.
Secondly, the solution is assessed as a self-contained piece of code that isn&rsquo;t used afterwards.</p>

<p>Professional software development seldom entails writing programmes from scratch that aren&rsquo;t refactored afterwards.</p>

<p>I propose a new method of assigning and grading take home tests that assesses those two qualities in candidates.</p>

<h2 id="assignment">Assignment</h2>

<p>The technical description of the assignment is outside the scope of this article.</p>

<p>Design a programming test that is representative of the problems in your domain and would be interesting to the candidate.</p>

<h3 id="what-kind-of-assignment-to-send">What kind of assignment to send?</h3>

<p>Send candidates an exercise to refactor an existing codebase to comply with a new spec or API.</p>

<p>The submission comes in a git repo.</p>

<p>Candidates need to commit their changes to the repo and send it back.</p>

<p>Include extraneous classes, endpoints and functions in the original.</p>

<p>We are testing the candidate&rsquo;s ability to read and understand someone else&rsquo;s code; refactor it to comply with new functionality, while reducing overall complexity.</p>

<h2 id="how-to-grade-it">How to grade it?</h2>

<p>The submissions is assessed in three stages: computer scoring, code review by an interviewer and pair programming onsite.
Computer scoring runs in a CI-like automated fashion with an environment consistent across languages and tools.
The reviewer marks how easy it is to extend the code base by adding new functionality.
During the onsite, the candidate pairs up with another interviewer to add new functionality to their submission.</p>

<p>Results include raw counts across several rubrics and the composite score.</p>

<p>You can compare raw scores across the same language/toolset configuration.</p>

<p>You shouldn&rsquo;t compare the number of linter warnings in Java submissions against that in Python submissions.</p>

<p>Overall, expect a good solution to delete more lines of code than it adds, remove now-unnecessary interfaces, implement requested features, while maintaining test coverage and resilience to bad inputs.</p>

<h3 id="computer-scoring-facets">Computer scoring facets</h3>

<p>Below is an outline of assessment facets broken down into individual factors and a suggested value function for each factor. You may change the individual value function and/or formula for the overall grade.</p>

<h4 id="functionality">Functionality</h4>

<ul>
<li>Number and percentage of unit tests passed</li>
<li>Number and percentage of integration tests passed. Call the API endpoints, whose implementation was requested.</li>
<li>Number of old endpoints that shouldn&rsquo;t exist anymore, but respond to inputs.</li>
</ul>

<h4 id="reliability-and-security">Reliability and security</h4>

<ul>
<li><p>Test coverage</p>

<p><strong>Suggested evaluation</strong></p>

<p><code>abs(90 - UNIT_TESTS_COVERAGE_PERCENT)</code> - lower is better.</p>

<p>Someone covering the test submission with 100% unit tests suggests inability of making trade-off decisions.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Error handling</p>

<p><strong>Suggested evaluation</strong></p>

<p>Number of API endpoints crashed by malformed inputs.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Resilience to bad inputs</p>

<p>This is <strong>mean</strong>, but if you care about security, your candidates might enjoy
this. Run a fuzzer over the API/application and count the number of errors.
Make sure all candidates get the same fuzzer and fuzzing inputs.</p></li>
</ul>

<h4 id="style-coherence-and-verbosity">Style, coherence and verbosity</h4>

<ul>
<li><p>Inserted vs deleted lines</p>

<p>Get the number of lines inserted and deleted in a git diff between your last commit and HEAD.</p>

<p><strong>Suggested evaluation</strong></p>

<p>Ratio as low as possible.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Number of linter warnings</p>

<p>Run a linter on all source files and record the summary of errors and
warnings. The configuration of the linter should be the same with the
linters you use on your code base at work.</p>

<p>If you are feeling mischievous, you might want to include something that
causes the linter to report errors in your original code and see if the
candidates correct your mistakes.</p>

<p>If you do this, make sure the linter is an obvious, easy-to-run
dependency. Otherwise you are punishing candidates for lack of familiarity
with your tool set.</p>

<p><strong>Suggested evaluation</strong></p>

<p>Fewer warnings is better.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Statistics on variable, method and class names</p>

<p>Extract names of all variables, methods and classes in the solution. Calculate  min, max, average and standard deviation of their lengths.</p>

<p>Make it more difficult by giving the functions and variables purposefully short names, before you send it to the candidate.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Ratio of changed lines per commits</p>

<p>It&rsquo;s possible to make many small commits that make unrelated changes.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Documentation</p>

<p>Count the number of classes and methods have a docstring. If you care about
assessing candidates&rsquo; writing ability, you might want to extract the text
in the docstrings and calculate its
<a href="https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid%5Freadability%5Ftests">readability</a>
score.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Ratio of LoC/methods</p>

<p>Using source-code analysis to calculate the average lines of code in all methods/functions in the submission.</p></li>
</ul>

<h4 id="performance">Performance</h4>

<p>Measure the mean and standard deviation of the following runs:</p>

<ul>
<li>Process 100,000 correct requests.</li>
<li>Process 100,000 malformed requests.</li>
<li>Process 50,000 correct and 50,000 malformed requests in random order.</li>
</ul>

<h4 id="passing-or-failing-candidates">Passing or failing candidates</h4>

<p>Use the facets above and devise a formula or a flow chart that allows you to confidently reject/pass candidates.</p>

<p>The most fair comparison is against the submissions completed by your engineers without prior knowledge of the test or assessment criteria.</p>

<p>Failed submissions get a rejection email. Generate rejection summary using the collected statistics. You don&rsquo;t want your criteria to leak online, so it&rsquo;s best not to give exact details.</p>

<h3 id="code-review-and-refactoring-by-an-interviewer">Code review and refactoring by an interviewer</h3>

<p>The interviewer assesses how easy it is to work with the candidate&rsquo;s code by
adding another endpoint that calls into methods that the candidate has
added.</p>

<p>The interviewer produces a qualitative assessment of the codebase.</p>

<p>To reduce bias, the reviewer cannot see the exact statistics from the previous section.</p>

<p>Even then, there are other factors that might affect the reviewer. After
grading dozens of submissions, the reviewer becomes more proficient with the
problem. This makes them capable of working with the code regardless of the
quality of the submission.</p>

<p>In the future, interviewers&rsquo; proficiency with the problem and experience
reviewing submissions might prove useful, when evaluating and adding new
assessment criteria.</p>

<h3 id="onsite-follow-up">Onsite follow up</h3>

<p>If the candidate comes onsite and ask them to pair programme with someone
from the team. Their task is to extend their original submission to include
new functionality.</p>

<p>You can ask them to add the same feature that the interviewer added, when
assessing the ease of working with your code base. Else, give them another
feature to implement. Give candidates explicit recommendation to re-read
their submission before the onsite, if their submission was a while ago.</p>

<h2 id="summary-of-trade-offs">Summary of trade-offs</h2>

<p>Below is a list of pros and cons of the suggested approach.</p>

<h3 id="benefits">Benefits</h3>

<p>Collecting standardised, consistent metrics across different dimensions enables the interviewers to avoid bias and get a detailed outline of candidate&rsquo;s skills.</p>

<h4 id="1-dot-even-closer-to-real-world-software-development">1. Even closer to real-world software development</h4>

<p>Assesses candidates&rsquo; ability to understand code, solve problems and deliver software that is easy to extend.</p>

<p>By solving an interesting problem, applicants become more interested in the job.</p>

<h4 id="2-dot-systematic-assessment-across-clear-predetermined-criteria">2. Systematic assessment across clear, predetermined criteria</h4>

<p>Once you agree and set up grading infrastructure, every submission will run
against the same battery of tests and populate an excel spreadsheet with
results.</p>

<h4 id="3-dot-ability-to-score-and-re-evaluate-new-submissions-against-those-of-previous-candidates">3. Ability to score and re-evaluate new submissions against those of previous candidates</h4>

<p>Keeping a history of all previous submissions allows us to add new metrics. After adding the new metric to the assessment framework, rerun it on all submissions to add new score.</p>

<p>including people that you extended offers to.</p>

<h4 id="4-dot-assess-different-aspects-with-the-same-test">4. Assess different aspects with the same test</h4>

<p>Different teams in the same company can use the same exercise but prioritise one rubric over all others.</p>

<p>For example, candidates applying for a role in the performance-focussed team are expected to submit solutions with above-average performance.</p>

<h3 id="disadvantages">Disadvantages</h3>

<h4 id="1-dot-too-close-to-real-world-software-development">1. Too close to real-world software development</h4>

<p>Candidates are people too, they want to relax in their
free time, not read and write more code. This might scare off
passive candidates.</p>

<p><strong>Mitigation strategies</strong></p>

<ul>
<li><p>Make sure the test can be completed in the advertised time</p>

<p>Measure how long it takes 3-5 developers in your company to do the test.</p>

<p>Picard management tip.</p>

<p>If you lure candidates in with a promise of &ldquo;only takes a couple of hours&rdquo; into a test
that took your team half a day, people end up feeling
disappointed with themselves and angry at you.</p>

<p>&ldquo;Scaring&rdquo; them with a purposefully pessimistic time estimate sets their
expectations higher. Completing it faster than expected gives them a
dopamine kick.</p>

<blockquote>
<p>&ldquo;X of your prospective team mates took between
{Y} and {Z} hours doing the test&rdquo;.format(X=coworkers_who_did_the_test, Y=min_time*1.5, Z=max_time*1.5)</p>
</blockquote></li>

<li><p>Sell the company and role to the candidate before the test</p>

<p>Organise a phone call to pitch the company and the role enough for candidates to want to do the take home test.</p>

<p>Cynically enough, you expect a large number of applicants to fail the test, so it might feel pointless to spend time selling the company.</p>

<p>However, pitching really well at the start of the process brings a secondary effect of making a great impression on all applicants.</p>

<p>Even the candidates, who fail come away and tell their friends about your company and interview process.</p>

<p>This makes it easier for you to hire their friends.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Make the assignment representative/interesting of your work</p>

<p>Find a problem specific to your company/domain and distil it down to the size of a take-home assignment.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Consider paying the candidates a nominal amount for their time.</p>

<p>They are doing a small, representative programming assignment and should be
paid around 20-50 GBP. The reward makes your application process
stand out and help you collect valuable data from a wider pool of applicants.</p>

<p>This doesn&rsquo;t work at Google-scale of millions of applicants per year, so plan accordingly.</p></li>
</ul>

<h4 id="2-dot-language-specific-assessment">2. Language-specific assessment</h4>

<p>Being more proficient with the language and its tools gives an unfair advantage to some candidates.</p>

<p>Candidates proficient in those languages are more likely to know about tools, code style and idiomatic way of writing code in that language.</p>

<p>You might care about proficiency with a specific language and even introduce language-specific criteria.</p>

<p>This filters out candidates that don&rsquo;t feel proficient enough in the language(s) offered.</p>

<p><strong>Mitigation strategies</strong></p>

<ul>
<li><p>Invest upfront into grading framework for 2 or 3 main languages that your company uses</p>

<p>If you use more languages than you can assess submissions in, maybe you should ask yourself why are you using that many languages in the first place.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Introduce language-independent assessment criteria</p>

<p>Performance is one of them.</p></li>
</ul>

<h4 id="3-dot-if-assessment-criteria-leak-online-it-might-skew-measurements">3. If assessment criteria leak online, it might skew measurements</h4>

<p>According to Goodhart&rsquo;s law once a measure becomes a target, it ceases to
be a good measure.</p>

<p>For example, telling candidates that we calculate the ratio of inserted
lines over deleted lines, might motivate them to play code golf to improve
this ratio.</p>

<p><strong>Mitigation strategies</strong></p>

<ul>
<li><p>Tell candidates about the multitude of criteria</p>

<p>Warn the candidates that their submissions runs through an automatic
assessment across several rubrics like testing, style and performance.
This gives participants fair warning and the opportunity to decide on the
trade-offs between time spent vs quality of submission.</p></li>
</ul>

<!--listend-->

<ul>
<li><p>Devise criteria to catch regressions across correlated variables</p>

<p>In the example above, the candidate playing code golf wrt lines of code
might resort to single letter variable names. Since we measure the length
of variable and method names, improving the lines metric is likely to
hurt the length of names metric.</p></li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Giving developer candidates a take home test is a more realistic way to gauge
their programming ability. Reading and modifying already existing code and
making sure it can be extended by someone else is one of the key skills for a
programmer. I suggest making it more enjoyable for the applicants and more
informative for the interviewers.</p>

<p>Overall, I suggest concentrating your interview process around the programming test.</p>

      
    </div>
    
  </div>
</section>


<section class="section">
  <div class="container has-text-centered">
    <p></p>
    
  </div>
</section>



</body>
</html>

